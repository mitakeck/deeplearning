{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# パラメタ\n",
    "learning_rate = 0.001\n",
    "training_epochs = 200000\n",
    "batch_size = 64\n",
    "display_step = 100\n",
    "\n",
    "# ネットワークパラメタ\n",
    "dropout = 0.75\n",
    "NUM_CLASSES = 10\n",
    "CHANNEL = 1\n",
    "IMAGE_SIZE = 64\n",
    "IMAGE_PIXELS = IMAGE_SIZE * IMAGE_SIZE * CHANNEL\n",
    "\n",
    "x = tf.placeholder(tf.float32, [None, IMAGE_PIXELS])\n",
    "y = tf.placeholder(tf.float32, [None, NUM_CLASSES])\n",
    "keep_prob = tf.placeholder(tf.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10, 4096)\n",
      "(10, 10)\n"
     ]
    }
   ],
   "source": [
    "# データセット\n",
    "csv_name = 'font_data64/random-all-head10.csv'\n",
    "fonts = [\"futura\", \"gillsans\", \"helvetica\", \"opitma\", \"andalemono\", \"arial\", \"impact\", \"timenewroman\", \"trebuchetms\", \"verdana\"]\n",
    "train_image = []\n",
    "train_label = []\n",
    "\n",
    "with open(csv_name, 'r') as f:\n",
    "    for line in f:\n",
    "        line = line.rstrip()\n",
    "        l = line.split(\",\")\n",
    "        img = cv2.imread(\"font_data64/\" + l[0], 0)\n",
    "#         img = cv2.equalizeHist(img)\n",
    "#         img = cv2.split(img)[1]\n",
    "#         img = cv2.resize(img, (IMAGE_SIZE, IMAGE_SIZE))\n",
    "#         img = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "#         train_image.append(img.flatten().astype(np.float32)/255.0)\n",
    "#         img = cv2.resize(img, (IMAGE_SIZE*IMAGE_SIZE, 1))[0]\n",
    "#         img = np.vstack(img)\n",
    "        cv2.imshow(\"img\", img)\n",
    "        cv2.waitKey(0)\n",
    "#         img = cv2.resize(img, (IMAGE_SIZE*IMAGE_SIZE, 1))\n",
    "        img = img.flatten().astype(np.float32)/256.0\n",
    "#         cv2.imshow(\"img\", img)\n",
    "#         cv2.waitKey(0)\n",
    "        cv2.imshow(\"img\", cv2.resize(img, (IMAGE_SIZE, IMAGE_SIZE)))\n",
    "        cv2.waitKey(0)\n",
    "        train_image.append(img)\n",
    "#         train_image.append(img)\n",
    "        tmp = np.zeros(NUM_CLASSES)\n",
    "        tmp[fonts.index(l[1])] = 1\n",
    "        train_label.append(tmp)\n",
    "    train_image = np.asarray(train_image)\n",
    "    train_label = np.asarray(train_label)\n",
    "    train_len = len(train_image)\n",
    "\n",
    "print train_image.shape\n",
    "print train_label.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.  0.  0.  0.  0.  1.  0.  0.  0.  0.]\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP4AAAD8CAYAAABXXhlaAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nrprzugAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x11a1f8310>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "img = train_image[0]\n",
    "img = cv2.resize(img, (IMAGE_SIZE, IMAGE_SIZE))\n",
    "plt.imshow(img)\n",
    "print train_label[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# 畳み込み処理\n",
    "def conv2d(x, W, b, strides=1):\n",
    "    x = tf.nn.conv2d(x, W, strides=[1, strides, strides, 1], padding=\"SAME\")\n",
    "    x = tf.nn.bias_add(x, b)\n",
    "    return tf.nn.relu(x)\n",
    "\n",
    "# プーリング処理\n",
    "def maxpool2d(x, k=2):\n",
    "    return tf.nn.max_pool(x, ksize=[1, k, k, 1], strides=[1, k, k, 1], padding=\"SAME\")\n",
    "\n",
    "# モデル定義\n",
    "def conv_net(x, weights, biases, dropout):\n",
    "    x = tf.reshape(x, [-1, IMAGE_SIZE, IMAGE_SIZE, CHANNEL])\n",
    "    \n",
    "    # 畳み込み1\n",
    "    conv1 = conv2d(x, weights[\"wc1\"], biases[\"bc1\"])\n",
    "    conv1 = maxpool2d(conv1, k=2)\n",
    "    \n",
    "    # 畳み込み2\n",
    "    conv2 = conv2d(conv1, weights[\"wc2\"], biases[\"bc2\"])\n",
    "    conv2 = maxpool2d(conv2, k=2)\n",
    "    \n",
    "    # 全結合\n",
    "    fc1 = tf.reshape(conv2, [-1, weights[\"wd1\"].get_shape().as_list()[0]])\n",
    "    fc1 = tf.add(tf.matmul(fc1, weights[\"wd1\"]), biases[\"bd1\"])\n",
    "    fc1 = tf.nn.relu(fc1)\n",
    "    fc1 = tf.nn.dropout(fc1, dropout) # dropout を適応\n",
    "    \n",
    "    out = tf.add(tf.matmul(fc1, weights[\"out\"]), biases[\"out\"])\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# 重み、バイアス管理用の変数定義\n",
    "weights = {\n",
    "    \"wc1\": tf.Variable(tf.random_normal([5, 5,  1, 32])),\n",
    "    \"wc2\": tf.Variable(tf.random_normal([5, 5, 32, 64])),\n",
    "    \"wd1\": tf.Variable(tf.random_normal([8*8*64, 1024])),\n",
    "    \"out\": tf.Variable(tf.random_normal([1024, NUM_CLASSES]))\n",
    "}\n",
    "biases = {\n",
    "    \"bc1\": tf.Variable(tf.random_normal([32])),\n",
    "    \"bc2\": tf.Variable(tf.random_normal([64])),\n",
    "    \"bd1\": tf.Variable(tf.random_normal([1024])),\n",
    "    \"out\": tf.Variable(tf.random_normal([NUM_CLASSES]))\n",
    "}\n",
    "\n",
    "# モデル\n",
    "pred = conv_net(x, weights, biases, keep_prob)\n",
    "\n",
    "# ロス関数\n",
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=pred, labels=y))\n",
    "# オプティマイザ\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost)\n",
    "\n",
    "# 評価用モデル\n",
    "correct_pred = tf.equal(tf.argmax(pred, 1), tf.argmax(y, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n",
    "\n",
    "# 初期化\n",
    "init = tf.global_variables_initializer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": false
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Cannot feed value of shape (64, 64, 64) for Tensor u'Placeholder_3:0', which has shape '(?, 4096)'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-92-14502a37a1c0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m         \u001b[0mbatch_x\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_image\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mbatch_plus\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[0mbatch_y\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_label\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mbatch_plus\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m         \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbatch_x\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbatch_y\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeep_prob\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mdropout\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mdisplay_step\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    765\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    766\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 767\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    768\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    769\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    942\u001b[0m                 \u001b[0;34m'Cannot feed value of shape %r for Tensor %r, '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    943\u001b[0m                 \u001b[0;34m'which has shape %r'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 944\u001b[0;31m                 % (np_val.shape, subfeed_t.name, str(subfeed_t.get_shape())))\n\u001b[0m\u001b[1;32m    945\u001b[0m           \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgraph\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_feedable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msubfeed_t\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    946\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Tensor %s may not be fed.'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0msubfeed_t\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Cannot feed value of shape (64, 64, 64) for Tensor u'Placeholder_3:0', which has shape '(?, 4096)'"
     ]
    }
   ],
   "source": [
    "# 実行\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    \n",
    "    step = 1\n",
    "    while step * batch_size < training_epochs:\n",
    "        batch = step * batch_size\n",
    "        batch_plus = (step+1) * batch_size\n",
    "\n",
    "        batch_x = train_image[batch:batch_plus]\n",
    "        batch_y = train_label[batch:batch_plus]\n",
    "        sess.run(optimizer, feed_dict={x: batch_x, y: batch_y, keep_prob: dropout})\n",
    "    \n",
    "        if (step+1) % display_step == 0:\n",
    "            loss, acc = sess.run([cost, accuracy], feed_dict={x: batch_x, y: batch_y, keep_prob: 1.})\n",
    "            print \"Epoch:\", str(step*batch_size), \"Loss=\", loss, \"Acc=\", acc\n",
    "        \n",
    "        step += 1\n",
    "\n",
    "    print \"Optimization Finish\"\n",
    "#     print \"Testing Accuracy:\", \\\n",
    "#     sess.run(accuracy, feed_dict={x: mnist.test.images[:256],\n",
    "#                                   y: mnist.test.labels[:256],\n",
    "#                                   keep_prob: 1.})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
